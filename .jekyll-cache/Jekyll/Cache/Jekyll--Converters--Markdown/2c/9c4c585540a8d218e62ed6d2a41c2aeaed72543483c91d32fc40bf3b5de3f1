I"±.<h3 id="background-and-rationale">Background and Rationale</h3>
<p>Mobile phones, wearable devices, voice assistants, and autonomous vehicles are just a few of the new
distributed networks generating a wealth of data each day, where each data sample belongs to different type of statistical distribution(Non- IIDâ€™s) . Due to the growing computational power of these devicesâ€”coupled with concerns about transmitting private information(if it gets leaked, a lot about the device and the userâ€™s behavior can be inferred easily)â€”it is increasingly attractive to store data locally and thus, push network computation closer to the edge devices. The learning becomes more challenging as it seems if data contains sensitive information like location, health, and other ambient signals because the private information gets more sensitive over time, which may lead to bad user experience. <strong><em>Federated learning</em></strong> <em>has emerged as a new training paradigm in such settings. Federated learning (aka collaborative learning) is a machine learning technique that trains an algorithm across multiple decentralized edge devices or servers holding local data samples, without exchanging their data samples</em>. It stands in contrast to traditional centralized machine learning techniques where all data samples are uploaded to one server, as well as to more classical decentralized approaches which assume that local data samples are identically distributed. From <a href="#references"><span style="color:green">[1]</span></a>, FL is privacy-preserving model training in heterogeneous,distributed networks.</p>

<h4 id="fl-applications">FL Applications</h4>
<p align="center">
<img src="https://move47.github.io/assets/Blog/FL/fl_introduction.png" />
</p>
<p align="center"><i>Auto-suggestion application</i></p>
<p><br /><br /></p>
<p align="center">
<img src="https://move47.github.io/assets/Blog/FL/fl_medical.png" />
</p>
<p align="center"><i>Usage of federated learning for providing better healthcare by taking relevant data from various medical organisations, which in turn is difficult to achieve through centralised learning.</i></p>
<p><br /><br /></p>

<h3 id="standard-federated-training-algorithm--">Standard Federated Training Algorithm :-</h3>
<p><strong>Step I</strong>. The master device sends the current global model parameters to all worker devices.</p>

<p><strong>Step II</strong>. The worker devices update their local model parameters using the current global model parameters and their local training datasets in parallel. In particular, each worker device essentially aims to solve the loss function using the stochastic gradient descent. Note that a worker device could apply <strong><em>SGD</em></strong> multiple rounds to update its local model. After updating the local models, the worker devices send them to the master device.</p>

<p><strong>Step III</strong>. The master device aggregates the local models from the worker devices to obtain a new global model according to a certain aggregation rule. <em>REPEAT STEP I.</em></p>

<h3 id="some-literature-review-about-attacks-on-fl">Some literature review about attacks on FL</h3>
<p>If we see the federated training algorithm closely, in all the three steps, the data of each individual is not being used directly, which motivates the participating users to share their sensitive data for useful learning. At first glance, it also provides some privacy guarantee as the raw data never leaves the user device, and only updates to models (e.g., gradient updates) are sent to the central server, which is not in the case of a centralized setting. On the contrary,  recent works by <em>Shokri et al</em> <span style="color:green"><a href="#references">[4]</a></span> have shown that it is possible to construct scenarios in which information about the raw data is leaked from a client to the server, such as a scenario <em>where knowing the previous model and the gradient update from a user would allow one to infer a training example held by that user</em> which is popularly known as <strong><em>Membership-Inference attack</em></strong> among ML community.</p>

<p>To provide better user privacy, several cryptographic and data perturbation techniques like 
<em>Homomorphic Encryption, Secure-Multi Party Computation, and Differential Privacy</em>(or any combination of them) are proposed, which do provide a reliable theoretical privacy guarantee. <em>Bargav et al.</em> <span style="color:green"><a href="#references">[3]</a></span>  presented a better lower bound in Differential Privacy as compared to other state-of-art bounds by perturbing the gradient at MPC i.e., aggregation side instead of adding noise at user level for Logistic regression. However the work didnâ€™t show any bounds for Deep learning algorithms which are mostly used in various learning tasks now-a-days. <em>Bargav et al.</em> <span style="color:green"><a href="#references">[2]</a></span> also shows the evaluation of the different versions of DP like <em>Naive DP, Renyi DP, etc.</em> on several well-known datasets trained using both traditional ML and Neural Networks algorithms and found out <em>privacy loss is directly proportional to number of epochs</em>. The <strong>Secure Aggregation protocol</strong> from <em>McMahan et al.</em> <span style="color:green"><a href="#references">[5]</a></span> has strong privacy guarantees when aggregating client updates. Moreover, the protocol is tailored to the setting of federated learning. For example,
it is robust to clients dropping out during the execution (a common feature of cross-device FL) and scales to
a large number of parties and vector lengths. However, this approach has several limitations: (a) it assumes a
semi-honest server (only in the private key infrastructure phase), (b) it allows the server to see the per-round
aggregates (which may still leak information), (c) it is not efficient for sparse vector aggregation, and (d)
it lacks the ability to enforce well-formedness of client inputs. Furthermore <span style="color:green"><a href="#references">[1]</a></span> , also provides the concept of FHE, which suits better in case of heavy computational edge devices, but fails miserably where end devices are Mobile phones or IOT devices, which of-course is the case of federated learning.</p>

<p>In the distributed datacenter and centralized settings, there has been a wide variety of work concerning
attacks and defenses for various <em>targetted</em> and <em>untargetted</em> attacks, namely through <strong>model update poisoning, data poisoning.</strong> for example, (an adversary might attempt to prevent a model from being learned at all). In these attacks, the adversary(or adversaries) directly control some number of clients, and thus, they can directly manipulate reports to service provider to alter the outputs to bias the learned model towards their objective. They may also be able to tailor their output to have similar variance and magnitude as the correct model updates, making them difficut to detect. For this, several <em>Byzantine-Resilient Defense</em> mechanisms were proposed by machine learning and Blockchain community, one of them is to replace the averaging step on the server with median-based agreegators. <em>Fang et al.</em> <span style="color:green"><a href="#references">[7]</a></span> performs successful local model poisoing attacks, which can substantially increase the error rates
of the models learnt through federated learning that
were claimed to be robust against Byzantine failures of some
client devices.</p>
<h3 id="relationship-between-model-update-poisoning-and-data-poisoning">Relationship between model update poisoning and data poisoning</h3>
<p>Intuitively, the relation between model update poisoning and data poisoning
should depend on the over-parameterization of the model with respect to the data. The study by <em>Fang et al.</em> <span style="color:green"><a href="#references">[6]</a></span> proposes two methods to defend against local model poisoning attacks. Their results clearly show that only in some cases, one out of two methods can effectively
defend these attacks and it also make the global model slower to learn
and adapt to new data as that data may be identified as from
potentially malicious local models. Thus, the proposed defenses are not effective enough for every case, highlighting the need for
new defenses against model poisoning attacks to
federated learning. Existing defenses against backdoor attacks either require a careful examination of the training data, or full control of the training process at the server (as happens in centralised setting), none of which may hold in the federated learning setting. <em>The property of Zero-knowledge proof can be used to ensure that users are submitting updates with pre-decided protocol</em>. One interesting line of study would be to quantify the gap between these two types of attacks, and relate
this gap to the relative strength of an adversary operating under these attack models. For example, the maximum number of clients that can
perform data poisoning attacks may be much higher than the number that can perform model update poisoning attacks, especially in cross-device settings. Thus, understanding the relation between these two attack
types, especially as they relate to the number of adversarial clients, would greatly help our understanding of
the threat landscape in federated learning.</p>
<h3 id="motivation-while-writing-this-post">Motivation while writing this post</h3>
<p>I had the following motivations while writing this post:-<br />
Given <strong><em>n</em></strong>, no. of working devices and an honest 
but curious Server, Formulation of an innovative protocol which can provide rigorous privacy guarantees in the following scenarios:-</p>
<ul>
  <li>When m out of n clients become adversaries or an adversary controls m clients in the cross-device or cross-silo federated setting, aiming to perform various attacks such as Data Poisoning, Model UpdatePoisoning, and Backdoor attacks.</li>
  <li>Robustness of the same developed protocol towards federated training algorithm, when a certain number of trusted users drop out due to participation constraints, ending up in a network having more number of malicious clients.</li>
</ul>

<h3 id="references">References</h3>
<ol>
  <li>David Evans, Rachel Cummings, Martin Jaggi <em>et al .</em>  <a href="https://arxiv.org/pdf/1912.04977.pdf"> Advances and Open problems in Federated Learning. </a><em>arXiv:1912.04977</em></li>
  <li>Bargav Jayaraman and David Evans. <a href="https://arxiv.org/pdf/1902.08874.pdf">Evaluating Differentially Private Machine Learning in Practice</a>.  <em>In 28th USENIX Security Symposium 2019</em>.</li>
  <li>Bargav Jayaraman, Lingxiao Wang, David Evans, and Quanquan Gu. <a href="http://papers.nips.cc/paper/7871-distributed-learning-without-distress-privacy-preserving-empirical-risk-minimization.pdf">Distributed Learning without Distress:
Privacy-Preserving Empirical Risk Minimization.</a> <em>32nd Conference on Neural Information Processing Systems (NeurIPS 2018).</em></li>
  <li>Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. <a href="https://www.cs.cornell.edu/~shmat/shmat_oak17.pdf">Membership Inference Attacks Against
Machine Learning Models. </a><em>IEEE Symposium on Security and Privacy (S\&amp;P) â€“ Oakland, 2017.</em></li>
  <li>Keith Bonawitz, H. Brendan McMahan, et al. (@Google), and Antonio Marcedone(@Cornell tech) . <a href="https://eprint.iacr.org/2017/281.pdf">Practical Secure Aggregation for Privacy-Preserving Machine Learning. </a><em>arXiv:1611.04482.</em></li>
  <li>Minghong Fang, Xiaoyu Cao, Jinyuan Jia, and Neil Zhenqiang Gomg. <a href="https://arxiv.org/pdf/1911.11815.pdf">Local Model Poisoning Attacks to Byzantine-Robust Federated Learning. </a><em>Usenix Security Symposium,2020.</em></li>
</ol>
:ET