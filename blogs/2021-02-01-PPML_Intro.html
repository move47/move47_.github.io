<!DOCTYPE html>
<html>

<head>
    
    <meta charset='utf-8'>
    <script type="module" src="https://cdn.jsdelivr.net/gh/zerodevx/zero-md@2/dist/zero-md.min.js"></script>
    <meta http-equiv='X-UA-Compatible' content='IE=edge'>
    <title>Blog</title>
    <link rel="stylesheet" href="https://unicons.iconscout.com/release/v4.0.0/css/line.css">
    <meta name='viewport' content='width=device-width, initial-scale=1'>
    <link rel='stylesheet' type='text/css' media='screen' href='../assets/css/styles.css'>
    <script src='../assets/js/main.js'></script>
    <!-- <style>
        li {
            margin-top: 2rem;
        }
        
        .blogbody{
            background-color: black;
            color: white;
        }
    </style> -->
</head>

<body>

    <div class="blogHeader">
        <div class="blogtitle">
            Privacy Preserving Machine Learning
        </div>
    </div>

    <div class="bcontent" style="margin-top: 2rem; background-color: white;">
        <div class="blogbody" style="padding: 2rem;">
            <!-- <zero-md  src="../_posts/2019-09-08-PPML_Intro.md" no-shadow=""></zero-md> -->
            <h3 id="introduction">Introduction</h3>
            <p><em>Arthur Samuel</em>, a pioneer in the field of computer gaming and artificial intelligence, described
                <strong>machine learning</strong>
                as a <em>field of study that gives computers the ability to learn without being explicitly
                    programmed</em>. ML is being
                increasingly utilized for a variety of applications from recommending new movies to programming
                self-driving
                cars. Furthermore, owing to it’s breakthrough in computer vision, speech recognition, ads, social feed
                and
                auto-complete etc, Deep Learning (DL) has gained huge attention and has helped the human race in many
                re-
                spects. With the advancement in technology, we can extend the embedment of these sophisticated
                applications
                into mobile devices and computers, with which individuals spend most of their time. Our activities like
                search
                queries, purchase transactions, the videos we watch, and our movie preferences are the few types of
                information
                that are being collected and stored on regular basis at both local(search engine) and remote
                storage(service
                enabled servers) for further processing. <strong><em>Our personal data generated through these
                        activities are used by ML
                        applications to give us better user-experience.</em></strong> Such private data is uploaded to
                centralized locations in clear
                text for ML algorithms which in-turn extract patterns from them, and update models
                accordingly(<strong>federated
                    learning</strong>). The sensitivity of such data increases it’s risk of being maliciously used by an
                insider as well as
                outsider. Additionally, it is possible to gain insights into private data sets even if these data are
                anonymized
                and the ML models are inaccessible.
                A machine learning process consists of two phases viz. <em>training phase</em> and <em>evaluation
                    phase</em>. In the learn-
                ing phase, a model or classifier is built on a possibly large data set. In the second phase, the trained
                model
                is being queried and based on its result, it is further updated for better accuracy in the next
                time-step. For
                every ML task, three different roles are possible: the input party (data owners or contributors), the
                computation
                party and the result party. In such systems, the data owner(s) send their data to the computation party
                which
                perform the required ML task and deliver the output to the result party. Such output could be an ML
                model
                that the result party can utilize for testing new samples. In other cases, the computation party might
                keep the
                ML model, and perform the task of testing new samples submitted by the result party, and returning the
                test
                results to the result party. If all the three roles are played by the same entity, then privacy is
                naturally pre-
                served, however when these roles are distributed across two or more entities, then
                <strong>PRIVACY-ENHANCING
                    TECHNOLOGIES</strong> are needed.</p>
            <h4 id="reviewofdataprivacyin201819">Review of Data Privacy in 2018-19:-</h4>
            <p>2018 was a breakout year for privacy-preserving machine learning(PPML). This year’s news cycle had
                several
                major stories surface around data privacy, which makes it the most relevant year for privacy since the
                Snowden
                leaks in 2013. Data Privacy impacts our politics, security, businesses, relationships, health, and
                finances.</p>
            <p align="center">
                <img src="../assets/Blog/PPML/google_attack_graph.png" />
            </p>
            <p align="center"><i>Figure 1: Google Trends for “data privacy”, 2013 — 2019</i></p>
            <p><br /><br /></p>
            <h4 id="mlasaservicemlaas">ML-as-a-Service(MLaaS)</h4>
            <p>In this technology-driven world, most of the organizations have deployed
                their ML and DL based trained models into the cloud for better reliability and integrity. For a
                particular query
                from Data owners, they are both Computation Party and Results’ Party and this can affect the overall
                privacy
                of owners adversely. In fact, with all of the data that is collected from individuals around the world
                on daily
                basis, data owners might not be aware of how the data collected from them is being used (or misused),
                and in
                many cases, not even aware that some data types are being collected.</p>
            <p align="center">
                <img src="../assets/Blog/PPML/Cloud.jpg" />
            </p>
            <p align="center"><i>Figure 2: Most Machine learning today is done in the cloud</i></p>
            <p><br /><br /></p>
            <h4 id="whyweneedprivacyinmlalgorithmsastheyalreadyinvolvecomplexmathematicswhichmakesdecodinghard">Why we
                need privacy in ML algorithms as they already involve complex mathematics-which makes decoding hard?
            </h4>
            <p>About 2 years ago, Google researchers introduced a Skin disease classifier at the global level. They
                developed
                an application to take photos of skin, and run it to <strong>CNN(Convolutional Neural Network)</strong>
                to intimate about
                your visit to a dermatologist. It was also observed that the accuracy of prediction was an expert level.
                When the user queries the application, they do not make any changes(e.g. noise perturbation) to the
                input skin image. The service provided then takes it for further processing. </p>
            <p align="center">
                <img src="../assets/Blog/PPML/skin_cancer.png" />
            </p>
            <p align="center"><i>Figure 3: Skin Cancer Classifier</i></p>
            <p><br /><br />
                The same uploaded skin image may contain sensitive information about an individual. The results of this
                information leakage could be harmful in some cases.
                Under such circumstances, the ML service could potentially become unreliable. Therefore, we need privacy
                at both the input and output ends when we don’t have access
                to the model.</p>
            <p><strong>Note:</strong> This is one of the example on top of my head now. There are hundreds of others
                also, which directs affects user privacy.</p>
            <h3 id="somethreatswithmachinelearning">Some threats with Machine Learning</h3>
            <ul>
                <li>
                    <p><strong>Membership Inference Attack</strong></p>
                    <p>Given an ML model and a sample (adversary’s knowledge), membership inference attacks aim to
                        determine if the sample was a member of the training set used to build this ML model
                        (adversary’s
                        target). This attack could be used by an adversary to learn whether a certain individual’s
                        record was
                        used to train an ML model associated with a specific disease. Such attacks utilize the
                        differences in
                        the ML model predictions on samples that were used in the training set versus those that were
                        not
                        included.</p>
                </li>
                <li>
                    <p><strong>De-anonymisation Attacks (Re-Identification)</strong></p>
                    <p><em>Identification of the target sample from the anonymized dataset.</em>
                        Example: Usage of IMDB background knowledge to identify the Netflix records of known users.This
                        incident demonstrates that anonymization cannot reliably protect the privacy of individuals in
                        the face of strong adversaries.</p>
                </li>
                <li>
                    <p><strong>Model Inversion Attack</strong></p>
                    <p>Some ML algorithms produce models where explicit feature vectors are not stored in the ML model,
                        E.g. ridge regression or neural networks. Hence, the adversary’s knowledge would be limited to
                        either:
                        (a) an ML model with no stored feature vectors (whitebox access), or (b) only the responses
                        returned
                        by the computation party when the results’ party submits new testing samples (black-box access).
                        Here, the adversary’s target is creating feature vectors that resemble those used to create an
                        ML
                        model by utilizing the responses received from that ML model.</p>
                </li>
                <li>
                    <p><strong>Re-Construction Attack</strong></p>
                    <p>In this case, the adversary’s goal is reconstructing the raw private data by using their
                        knowledge
                        of the feature vectors. Reconstruction attacks require white-box access to the ML model, i.e.
                        the
                        feature vectors in a model must be known. Such attacks could be possible when the feature
                        vectors
                        used for the ML training phase were not deleted after building the desired ML model. In fact,
                        some
                        ML algorithms such as SVM or kNN store feature vectors in the model itself.).The final aim of
                        these
                        attacks is to misguide a ML system into thinking the reconstructed raw data belonged to a
                        certain
                        data owner.
                        Example: Fingerprint reconstruction, Mobile device touch gesture reconstruction.</p>
                </li>
            </ul>
            <p align="center">
                <img src="../assets/Blog/PPML/Attack.png" />
            </p>
            <p align="center"><i>Figure 4: Threats with ML Models</i></p>
            <p><br /><br /></p>
            <h3 id="somemethodsforprivateml">Some methods for Private ML</h3>
            <ul>
                <li><strong>Cryptographic approaches:</strong>
                    When a certain ML application requires data from multiple input parties,
                    cryptographic protocols could be utilized to perform ML training/testing on encrypted data. In many
                    of
                    these techniques, achieving better efficiency involved having data owners contribute their encrypted
                    data
                    to the computation servers.It can also be seen by training the model on cipher-text.
                    <ul>
                        <li>Homomorphic encryption</li>
                        <li>Secure Multiparty Computation(Secret sharing)</li>
                        <li>Garbled circuits etc.</li>
                    </ul>
                </li>
                <li><strong>Perturbation techniques:</strong>
                    Feeding updated data by adding noise to the actual raw data, which results into similar output as
                    before
                    without revealing anything about actual information.<ul>
                        <li>Differential Privacy<ul>
                                <li>Data Owners’ i.e. Input perturbation</li>
                                <li>Computational Party i.e. Algorithmic perturbation</li>
                                <li>Results’ Party i.e .Output perturbation</li>
                            </ul>
                        </li>
                        <li>Dimensional reduction</li>
                    </ul>
                </li>
            </ul>
        </div>

    </div>
    <div>

    </div>

    <iframe id="iFrameID" frameborder="0" style="visibility:hidden;" src="../footer.html"
        onload="this.before((this.contentDocument.body||this.contentDocument).children[0]);this.remove()"></iframe>


</body>

</html>